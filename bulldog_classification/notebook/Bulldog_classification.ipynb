{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulldog classification via ResNet-50 transfer learning\n",
    "\n",
    "Tutorial for bulldog classification via ResNet-50 transfer learning in pytorch by using the dataset imported from ABEJA Platform Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from abeja.datasets import Client as DatasetClient\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import data from ABEJA Platform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set credential\n",
    "credential = {\n",
    "    'user_id': 'user-XXXXXXXXXXXXX',\n",
    "    'personal_access_token': 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "}\n",
    "\n",
    "organization_id = 'XXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from ABEJA Platform Dataset\n",
    "def load_dataset_from_api(dataset):\n",
    "    for item in tqdm(dataset.dataset_items.list(prefetch=True)):\n",
    "        try:\n",
    "            file_content = item.source_data[0].get_content()\n",
    "            file_like_object = io.BytesIO(file_content)\n",
    "            img = Image.open(file_like_object).convert('RGB')\n",
    "            label_id = item.attributes['classification'][0]['label_id']\n",
    "            yield img, label_id\n",
    "        except OSError:\n",
    "            print('Fail to load dataset_item_id {}.'.format(item.dataset_item_id))\n",
    "\n",
    "# define dataset id\n",
    "dataset_id = 'XXXXXXXXXXXXX'\n",
    "\n",
    "# get dataset via ABEJA Platform api\n",
    "dataset_client = DatasetClient(credential=credential, organization_id=organization_id)\n",
    "dataset = dataset_client.get_dataset(dataset_id)\n",
    "dataset_list = list(load_dataset_from_api(dataset))\n",
    "num_classes = len(dataset.props['categories'][0]['labels'])\n",
    "print('number of classes is {}.'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline\n",
    "# show one of images\n",
    "for item in dataset_list:\n",
    "    plt.imshow(item[0])\n",
    "    plt.title('label id is {}'.format(item[1]))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define customized dataset class\n",
    "class CustomizedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_list, transform=None):\n",
    "        self.transform = transform\n",
    "        self.dataset_list = dataset_list\n",
    "        self.img = [item[0] for item in self.dataset_list]\n",
    "        self.label_id = [item[1] for item in self.dataset_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_img = self.img[idx]\n",
    "        out_label_id = self.label_id[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            out_img = self.transform(out_img)\n",
    "\n",
    "        return out_img, out_label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to a normalized torch.FloatTensor\n",
    "train_transform = transforms.Compose([transforms.Resize(size=256),\n",
    "                  transforms.CenterCrop((224, 224)),\n",
    "                  transforms.RandomHorizontalFlip(),  # randomly flip and rotate\n",
    "                  transforms.RandomRotation(10),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transform = transforms.Compose([transforms.Resize(size=256),\n",
    "                  transforms.CenterCrop((224, 224)),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_size = 0.2\n",
    "num_workers = 1\n",
    "batch_size = 12\n",
    "\n",
    "# prepare dataloader\n",
    "def load_split_train_valid(dataset_list):\n",
    "    \"\"\" Split dataset into training and validation set \"\"\"\n",
    "\n",
    "    train_data = CustomizedDataset(dataset_list, transform=train_transform)\n",
    "    valid_data = CustomizedDataset(dataset_list, transform=valid_transform)\n",
    "\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    trainloader = DataLoader(train_data, sampler=train_sampler,\n",
    "                             batch_size=batch_size, num_workers=num_workers)\n",
    "    validloader = DataLoader(valid_data, sampler=valid_sampler,\n",
    "                             batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    print('number of train datasets is {}.'.format(len(trainloader) * batch_size))\n",
    "    print('number of valid datasets is {}.'.format(len(validloader) * batch_size))\n",
    "\n",
    "    return trainloader, validloader\n",
    "\n",
    "# create dataloader\n",
    "trainloader, validloader = load_split_train_valid(dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# set save path\n",
    "save_path = './model.pt'\n",
    "\n",
    "# specify model architecture (ResNet-50)\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# replace the last fully connected layer with a Linnear layer with no. of classes out features\n",
    "model.fc = nn.Linear(2048, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainloader, validloader, model, optimizer, criterion):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = 3.877533  # np.Inf\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # initialize variables to monitor training and validation loss and accuracy\n",
    "        train_loss = 0.0\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "        valid_loss = 0.0\n",
    "        valid_total = 0\n",
    "        valid_correct = 0\n",
    "\n",
    "        # train the model\n",
    "        model.train()\n",
    "        for data, target in trainloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            # count number of correct labels\n",
    "            _, preds_tensor = torch.max(output, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (preds_tensor == target).sum().item()\n",
    "\n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        for data, target in validloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss\n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "            # count number of correct labels\n",
    "            _, preds_tensor = torch.max(output, 1)\n",
    "            valid_total += target.size(0)\n",
    "            valid_correct += (preds_tensor == target).sum().item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "        valid_loss /= len(validloader.dataset)\n",
    "        # calculate accuracy\n",
    "        train_acc = train_correct / train_total\n",
    "        valid_acc = valid_correct / valid_total\n",
    "\n",
    "        # print training/validation statistics\n",
    "        print('Epoch: {} \\tTrain loss: {:.6f} \\tTrain acc: {:.6f} \\tValid loss: {:.6f} \\tValid acc: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                train_loss,\n",
    "                train_acc,\n",
    "                valid_loss,\n",
    "                valid_acc))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model.'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "# train the model\n",
    "train_model(trainloader, validloader, model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "# load the model for CPU\n",
    "device = torch.device('cpu')\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(2048, num_classes)\n",
    "model.load_state_dict(torch.load(save_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(img):\n",
    "    img = img.convert('RGB')\n",
    "    transformations = transforms.Compose([transforms.Resize(size=256),\n",
    "                                          transforms.CenterCrop((224, 224)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "    image_tensor = transformations(img)[:3, :, :].unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "def decode_predictions(result):\n",
    "    categories = {\n",
    "        0: 'BOSTON_BULL',\n",
    "        1: 'BULL_MASTIFF',\n",
    "        2: 'FRENCH_BULLDOG',\n",
    "        3: 'STAFFORDSHIRE_BULLTERRIER'\n",
    "    }\n",
    "    result_with_labels = [{\"label\": categories[i], \"probability\": score} for i, score in enumerate(result)]\n",
    "    return sorted(result_with_labels, key=lambda x: x['probability'], reverse=True)\n",
    "\n",
    "\n",
    "def predict(img):\n",
    "    image_tensor = image_to_tensor(img)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    model.eval()\n",
    "    output = model(image_tensor)\n",
    "    # convert output probabilities to predicted class\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    preds_tensor = softmax(output)\n",
    "    result = np.squeeze(preds_tensor.to(device).detach().numpy())\n",
    "    sorted_result = decode_predictions(result.tolist())\n",
    "    return {\"result\": sorted_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./test_french_bull.jpg')\n",
    "predicted = predict(img)\n",
    "plt.title(\"label:{}  prob:{:.2%}\".format(predicted['result'][0]['label'],\n",
    "                                         predicted['result'][0]['probability']))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = Image.open('./test_bull_mastif.jpg')\n",
    "predicted = predict(img)\n",
    "plt.title(\"label:{}  prob:{:.2%}\".format(predicted['result'][0]['label'],\n",
    "                                         predicted['result'][0]['probability']))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
