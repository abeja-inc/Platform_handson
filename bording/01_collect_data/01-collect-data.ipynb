{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセット作成\n",
    "このNotebookでは、Kaggleで公開されている花の画像を利用し、\n",
    "ABEJA PlatformでImage Classificationを利用するためのデータセットを作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load data\n",
    "データセットを作成するためのデータをダウンロードします。\n",
    "このステップではKaggleで提供されている花のデータセットをダウンロードして利用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = '1w6Fn_ZjLlPzAswSRFj6uuusYod87apHJ'\n",
    "destination = './flowers-recognition.zip'\n",
    "download_file_from_google_drive(file_id, destination)\n",
    "print('Download OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ダウンロードした画像データを解凍・ファイルチェック\n",
    "ダウンロードしたデータをunzipし、ファイル数、フォルダ名をチェックします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip download data\n",
    "n_files = !unzip -l ./flowers-recognition.zip | grep .jpg | wc -l\n",
    "!unzip -o ./flowers-recognition.zip | pv -l -s {n_files[0]} > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# load filenames for images\n",
    "file_names = list(glob('./flowers/*/*'))\n",
    "dir_names = list(glob('./flowers/*'))\n",
    "\n",
    "# print number of images in dataset\n",
    "print('There are %d total images.' % len(file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select directories\n",
    "selected_dirnames = [d for d in dir_names]\n",
    "print(selected_dirnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: DataLakeにデータをアップロードします。\n",
    "このステップでは、ダウンロードしたデータにメタデータ(Label)を付与してDataLakeにアップロードします。\n",
    "※事前にDataLakeチャンネルの作成が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set credential\n",
    "credential = {\n",
    "    'user_id': 'user-XXXXXXXXXXXXX',\n",
    "    'personal_access_token': 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "}\n",
    "\n",
    "organization_id='XXXXXXXXXXXXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abeja.datalake import Client as DatalakeClient\n",
    "\n",
    "# set datalake channel_id\n",
    "channel_id = 'XXXXXXXXXXXXX'\n",
    "\n",
    "datalake_client = DatalakeClient(organization_id, credential)\n",
    "channel = datalake_client.get_channel(channel_id)\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# upload directory data to datalake\n",
    "for d in tqdm(selected_dirnames):\n",
    "    # convert to uppercase and remove numbers\n",
    "    label_name = os.path.basename(d)\n",
    "    metadata = {'label': label_name}\n",
    "    channel.upload_dir(d, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: データレイクに保存したファイルのメタデータ(Label)を利用してJSONを出力\n",
    "データセット作成に必要な「プロパティ」に入力するJSONをメタデータを利用して出力します。\n",
    "※出力されたJSONの最初と最後のカンマ以外をコピーして利用ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "labels = sorted([os.path.basename(d) for d in selected_dirnames])\n",
    "labels_and_id = []\n",
    "label_to_id = {}\n",
    "\n",
    "for i, name in enumerate(labels):\n",
    "    label_to_id[name] = i\n",
    "    labels_and_id.append({'label_id': i,\n",
    "                   'label': name})\n",
    "    \n",
    "# define category name\n",
    "category_name = 'flower-classificaiton'\n",
    "\n",
    "# create dataset label\n",
    "category = {\n",
    "    'category_id': 0,\n",
    "    'name': category_name,\n",
    "    'labels': labels_and_id}\n",
    "\n",
    "props = {'categories': [category]}\n",
    "json.dumps(props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Datalakeに保存したファイルからデータセットを作成します。\n",
    "このステップでは、DataLakeに保存したファイルを利用してデータセットを作成します。\n",
    "※ラベルについては、メタデータ(Label)で設定されている情報を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset by importing datalake files\n",
    "from abeja.datasets import Client as DatasetClient\n",
    "\n",
    "dataset_client = DatasetClient(organization_id, credential)\n",
    "\n",
    "# define dataset id\n",
    "dataset_id = 'XXXXXXXXXXXXX'\n",
    "\n",
    "dataset = dataset_client.get_dataset(dataset_id)\n",
    "\n",
    "for f in tqdm(channel.list_files()):\n",
    "    data_uri = f.uri\n",
    "    filename = f.metadata['filename']\n",
    "    label = f.metadata['label']\n",
    "    label_id = label_to_id[label]\n",
    "    \n",
    "    if os.path.splitext(filename)[1].lower() == '.jpg' or \\\n",
    "    os.path.splitext(filename)[1].lower() == '.jpeg':\n",
    "        content_type = 'image/jpeg'\n",
    "    elif os.path.splitext(filename)[1].lower() == '.png':\n",
    "        content_type = 'image/png'\n",
    "    else:\n",
    "        print('{} is invalid file type.'.format(filename))\n",
    "        continue\n",
    "    \n",
    "    source_data = [{'data_uri': data_uri, 'data_type': content_type}]\n",
    "    attributes = {'classification': [{'category_id': 0, 'label_id': label_id, 'label': label}]}\n",
    "    dataset.dataset_items.create(source_data, attributes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
