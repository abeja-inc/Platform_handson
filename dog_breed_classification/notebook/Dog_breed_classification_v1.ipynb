{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dog breed classification via ResNet-50 transfer learning version 1\n",
    "\n",
    "Tutorial for dog breed classification via ResNet-50 transfer learning in pytorch by using the dataset imported from ABEJA Platform Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Import data from ABEJA Platform Datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory\n",
    "notebook_id = %env TRAINING_NOTEBOOK_ID\n",
    "%cd '/mnt/notebooks/{notebook_id}/Platform_handson/dog_breed_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from ABEJA Platform Dataset\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "from abeja.datasets import Client\n",
    "\n",
    "dataset_id = 1744582570357\n",
    "client = Client(organization_id=1225098818583)\n",
    "dataset = client.get_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = dataset.dataset_items.list(prefetch=False)\n",
    "\n",
    "dataset_list_img_label = []\n",
    "\n",
    "for item in tqdm(dataset_list):\n",
    "    # get data and label_id\n",
    "    file_content = item.source_data[0].get_content()\n",
    "    file_like_object = io.BytesIO(file_content)\n",
    "    img = Image.open(file_like_object)\n",
    "    img = img.convert('RGB')\n",
    "    dataset_list_img_label.append([img, item.attributes['classification'][0]['label_id']])\n",
    "\n",
    "# print number of images in dataset\n",
    "print('There are %d total dog images.' % len(dataset_list_img_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Vasualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(img):\n",
    "    '''\n",
    "    As per Pytorch documentations: All pre-trained models expect input images normalized in the same way, \n",
    "    i.e. mini-batches of 3-channel RGB images\n",
    "    of shape (3 x H x W), where H and W are expected to be at least 224. \n",
    "    The images have to be loaded in to a range of [0, 1] and \n",
    "    then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. \n",
    "    '''\n",
    "    img = img.convert('RGB')\n",
    "    transformations = transforms.Compose([transforms.Resize(size=224),\n",
    "                                          transforms.CenterCrop((224,224)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "    image_tensor = transformations(img)[:3,:,:].unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "# helper function for un-normalizing an image\n",
    "# and converting it from a Tensor image to a NumPy image for display\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of label id and label name\n",
    "id_to_label = {}\n",
    "labels = []\n",
    "for item in dataset.props['categories'][0]['labels']:\n",
    "    id_to_label[item['label_id']] = item['label']\n",
    "    labels.append(item['label'].upper())\n",
    "\n",
    "print(\"Number of classes:\", len(id_to_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show one of images\n",
    "dog_image = dataset_list_img_label[0][0]\n",
    "plt.imshow(dog_image)\n",
    "plt.show()\n",
    "print(id_to_label[dataset_list_img_label[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show one of images after conversion\n",
    "test_tensor = image_to_tensor(dog_image)\n",
    "print(test_tensor.shape)\n",
    "plt.imshow(im_convert(test_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_list_img_label, transform=None):\n",
    "        self.transform = transform\n",
    "        self.dataset_list_img_label = dataset_list_img_label\n",
    "        self.img = []\n",
    "        self.label = []\n",
    "        for item in dataset_list_img_label:\n",
    "            self.img.append(item[0])\n",
    "            self.label.append(item[1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_img = self.img[idx]\n",
    "        out_label =  self.label[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            out_img = self.transform(out_img)\n",
    "        \n",
    "        return out_img, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples per batch to load\n",
    "batch_size = 16\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 2\n",
    "\n",
    "# difine size of validation data\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to a normalized torch.FloatTensor\n",
    "train_transform = transforms.Compose([transforms.Resize(size=224),\n",
    "                                transforms.CenterCrop((224,224)),\n",
    "                                transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
    "                                transforms.RandomRotation(10),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transform = transforms.Compose([transforms.Resize(size=224),\n",
    "                                transforms.CenterCrop((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_split_train_test(dataset_list_img_label, valid_size, batch_size):\n",
    "    \"\"\" Split dataset into train and validation set \"\"\"\n",
    "    \n",
    "    train_data = Dataset(dataset_list_img_label, transform=train_transform)\n",
    "    valid_data = Dataset(dataset_list_img_label, transform=valid_transform)\n",
    "    \n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    \n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=batch_size, num_workers=num_workers)\n",
    "    validloader = torch.utils.data.DataLoader(valid_data,\n",
    "                   sampler=valid_sampler, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader, validloader\n",
    "\n",
    "trainloader, validloader = load_split_train_test(dataset_list_img_label, valid_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of training data\n",
    "inputs, classes = next(iter(trainloader))\n",
    "\n",
    "for image, label in zip(inputs, classes): \n",
    "    image = image.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "     \n",
    "    fig = plt.figure(figsize=(12,3))\n",
    "    plt.imshow(image)\n",
    "    plt.title(id_to_label[label.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model architecture (ResNet-50)\n",
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# replace the last fully connected layer with a Linnear layer with 120 out features\n",
    "model.fc = nn.Linear(2048, 120)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, trainloader, validloader, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = 3.877533 #np.Inf\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training, validation loss and validation accuracy\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        # train the model\n",
    "        model.train()\n",
    "        for data, target in trainloader:\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        for data, target in validloader:\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "    \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            # count number of correct labels\n",
    "            _, preds_tensor = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (preds_tensor == target).sum().item()\n",
    "            \n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(trainloader.dataset)\n",
    "        valid_loss = valid_loss/len(validloader.dataset)\n",
    "        # calculate accuracy\n",
    "        valid_accuracy = correct/total\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation accuracy: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            valid_accuracy\n",
    "            ))\n",
    "        \n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "n_epochs = 5\n",
    "train(n_epochs, trainloader, validloader, model, optimizer, criterion, use_cuda, './model/dog_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# load the model for CPU\n",
    "device = torch.device('cpu')\n",
    "dog_model = models.resnet50(pretrained=True)\n",
    "\n",
    "# freeze parameters so we don't backprop through them\n",
    "for param in dog_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# replace the last fully connected layer with a Linnear layer with 120 out features\n",
    "dog_model.fc = nn.Linear(2048, 120)\n",
    "\n",
    "dog_model.load_state_dict(torch.load('./model/dog_model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(img):\n",
    "    '''\n",
    "    As per Pytorch documentations: All pre-trained models expect input images normalized in the same way, \n",
    "    i.e. mini-batches of 3-channel RGB images\n",
    "    of shape (3 x H x W), where H and W are expected to be at least 224. \n",
    "    The images have to be loaded in to a range of [0, 1] and \n",
    "    then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. \n",
    "    '''\n",
    "    img = img.convert('RGB')\n",
    "    transformations = transforms.Compose([transforms.Resize(size=224),\n",
    "                                          transforms.CenterCrop((224,224)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "    image_tensor = transformations(img)[:3,:,:].unsqueeze(0)\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of class names\n",
    "class_names = ['AFFENPINSCHER',\n",
    " 'AFGHAN_HOUND',\n",
    " 'AFRICAN_HUNTING_DOG',\n",
    " 'AIREDALE',\n",
    " 'AMERICAN_STAFFORDSHIRE_TERRIER',\n",
    " 'APPENZELLER',\n",
    " 'AUSTRALIAN_TERRIER',\n",
    " 'BASENJI',\n",
    " 'BASSET',\n",
    " 'BEAGLE',\n",
    " 'BEDLINGTON_TERRIER',\n",
    " 'BERNESE_MOUNTAIN_DOG',\n",
    " 'BLACK-AND-TAN_COONHOUND',\n",
    " 'BLENHEIM_SPANIEL',\n",
    " 'BLOODHOUND',\n",
    " 'BLUETICK',\n",
    " 'BORDER_COLLIE',\n",
    " 'BORDER_TERRIER',\n",
    " 'BORZOI',\n",
    " 'BOSTON_BULL',\n",
    " 'BOUVIER_DES_FLANDRES',\n",
    " 'BOXER',\n",
    " 'BRABANCON_GRIFFON',\n",
    " 'BRIARD',\n",
    " 'BRITTANY_SPANIEL',\n",
    " 'BULL_MASTIFF',\n",
    " 'CAIRN',\n",
    " 'CARDIGAN',\n",
    " 'CHESAPEAKE_BAY_RETRIEVER',\n",
    " 'CHIHUAHUA',\n",
    " 'CHOW',\n",
    " 'CLUMBER',\n",
    " 'COCKER_SPANIEL',\n",
    " 'COLLIE',\n",
    " 'CURLY-COATED_RETRIEVER',\n",
    " 'DANDIE_DINMONT',\n",
    " 'DHOLE',\n",
    " 'DINGO',\n",
    " 'DOBERMAN',\n",
    " 'ENGLISH_FOXHOUND',\n",
    " 'ENGLISH_SETTER',\n",
    " 'ENGLISH_SPRINGER',\n",
    " 'ENTLEBUCHER',\n",
    " 'ESKIMO_DOG',\n",
    " 'FLAT-COATED_RETRIEVER',\n",
    " 'FRENCH_BULLDOG',\n",
    " 'GERMAN_SHEPHERD',\n",
    " 'GERMAN_SHORT-HAIRED_POINTER',\n",
    " 'GIANT_SCHNAUZER',\n",
    " 'GOLDEN_RETRIEVER',\n",
    " 'GORDON_SETTER',\n",
    " 'GREAT_DANE',\n",
    " 'GREAT_PYRENEES',\n",
    " 'GREATER_SWISS_MOUNTAIN_DOG',\n",
    " 'GROENENDAEL',\n",
    " 'IBIZAN_HOUND',\n",
    " 'IRISH_SETTER',\n",
    " 'IRISH_TERRIER',\n",
    " 'IRISH_WATER_SPANIEL',\n",
    " 'IRISH_WOLFHOUND',\n",
    " 'ITALIAN_GREYHOUND',\n",
    " 'JAPANESE_SPANIEL',\n",
    " 'KEESHOND',\n",
    " 'KELPIE',\n",
    " 'KERRY_BLUE_TERRIER',\n",
    " 'KOMONDOR',\n",
    " 'KUVASZ',\n",
    " 'LABRADOR_RETRIEVER',\n",
    " 'LAKELAND_TERRIER',\n",
    " 'LEONBERG',\n",
    " 'LHASA',\n",
    " 'MALAMUTE',\n",
    " 'MALINOIS',\n",
    " 'MALTESE_DOG',\n",
    " 'MEXICAN_HAIRLESS',\n",
    " 'MINIATURE_PINSCHER',\n",
    " 'MINIATURE_POODLE',\n",
    " 'MINIATURE_SCHNAUZER',\n",
    " 'NEWFOUNDLAND',\n",
    " 'NORFOLK_TERRIER',\n",
    " 'NORWEGIAN_ELKHOUND',\n",
    " 'NORWICH_TERRIER',\n",
    " 'OLD_ENGLISH_SHEEPDOG',\n",
    " 'OTTERHOUND',\n",
    " 'PAPILLON',\n",
    " 'PEKINESE',\n",
    " 'PEMBROKE',\n",
    " 'POMERANIAN',\n",
    " 'PUG',\n",
    " 'REDBONE',\n",
    " 'RHODESIAN_RIDGEBACK',\n",
    " 'ROTTWEILER',\n",
    " 'SAINT_BERNARD',\n",
    " 'SALUKI',\n",
    " 'SAMOYED',\n",
    " 'SCHIPPERKE',\n",
    " 'SCOTCH_TERRIER',\n",
    " 'SCOTTISH_DEERHOUND',\n",
    " 'SEALYHAM_TERRIER',\n",
    " 'SHETLAND_SHEEPDOG',\n",
    " 'SHIH-TZU',\n",
    " 'SIBERIAN_HUSKY',\n",
    " 'SILKY_TERRIER',\n",
    " 'SOFT-COATED_WHEATEN_TERRIER',\n",
    " 'STAFFORDSHIRE_BULLTERRIER',\n",
    " 'STANDARD_POODLE',\n",
    " 'STANDARD_SCHNAUZER',\n",
    " 'SUSSEX_SPANIEL',\n",
    " 'TIBETAN_MASTIFF',\n",
    " 'TIBETAN_TERRIER',\n",
    " 'TOY_POODLE',\n",
    " 'TOY_TERRIER',\n",
    " 'VIZSLA',\n",
    " 'WALKER_HOUND',\n",
    " 'WEIMARANER',\n",
    " 'WELSH_SPRINGER_SPANIEL',\n",
    " 'WEST_HIGHLAND_WHITE_TERRIER',\n",
    " 'WHIPPET',\n",
    " 'WIRE-HAIRED_FOX_TERRIER',\n",
    " 'YORKSHIRE_TERRIER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_breed(img, use_cuda=False):\n",
    "    \"\"\" load the image and return the predicted breed \"\"\"\n",
    "    image_tensor = image_to_tensor(img)\n",
    "    \n",
    "    # get outputs\n",
    "    if use_cuda:\n",
    "        image_tensor = image_tensor.cuda()\n",
    "    \n",
    "    dog_model.eval()\n",
    "    output = dog_model(image_tensor)\n",
    "    \n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    pred = np.squeeze(preds_tensor.numpy()) if not use_cuda else np.squeeze(preds_tensor.cpu().numpy())\n",
    "    result = class_names[pred]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def display_image(img, title=\"Title\"):\n",
    "    plt.title(title)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "def dog_classifier(img):\n",
    "    predicted_breed = predict_breed(img)\n",
    "    display_image(img, title=f\"Predicted:{predicted_breed}\")\n",
    "        \n",
    "    print(\"Your breed is most likley ...\")\n",
    "    print(predicted_breed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline\n",
    "\n",
    "img = Image.open('./sample/test_french_bull.jpg')\n",
    "dog_classifier(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./sample/test_bull_mastif.jpg')\n",
    "dog_classifier(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./sample/test_Labrador.jpg')\n",
    "dog_classifier(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
